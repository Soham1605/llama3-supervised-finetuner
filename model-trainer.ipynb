{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWFkIjk93bbI"
      },
      "source": [
        "# Math Question Answer Verification Competition\n",
        "\n",
        "## TEAM LLM training notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifjFmLe63bbK"
      },
      "source": [
        " ## INSTALLATION\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "38sPfDnU55Pm"
      },
      "outputs": [],
      "source": [
        "# Installations without capture\n",
        "# to see the output\n",
        "!pip install uv\n",
        "\n",
        "!uv pip install  \\\n",
        "  \"unsloth @ git+https://github.com/unslothai/unsloth.git\" \\\n",
        "  \"xformers\" \"trl\" \"peft\" \\\n",
        "  \"accelerate\" \"bitsandbytes\" \"transformers\"\n",
        "!uv pip install unsloth_zoo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3fa-KD0-3bbL"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "# We tried:\n",
        "# 1024 - did well, some samples were of higher sequence length\n",
        "# 2048 - excessive, produced 62% accuracy\n",
        "# 4094 - too high, no samples have such a high seq length\n",
        "# 8192 - didn't need to try it at all, RAM crash!\n",
        "max_seq_length = 1048 # to fit all samples :)\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cjgdfjwt3bbL"
      },
      "outputs": [],
      "source": [
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Meta-Llama-3.1-8B\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "liUtvaEy3bbL"
      },
      "source": [
        "## Training Prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LwzIHfgT3bbL"
      },
      "outputs": [],
      "source": [
        "# We changed the training prompt to focus more on the solution\n",
        "# The model did well even with the prompt in the starter notebook\n",
        "# but this is even better\n",
        "training_prompt = \"\"\"You are a meticulous math expert. Your goal is to verify the *entire reasoning* of the solution, not just the final answer.\n",
        "Read the problem, then carefully analyze each step of the solution for logical errors, calculation mistakes, or incorrect reasoning.\n",
        "Based on your step-by-step analysis, determine if the solution is correct.\n",
        "\n",
        "Answer with ONLY True or False.\n",
        "\n",
        "### Problem:\n",
        "{}\n",
        "\n",
        "### Solution:\n",
        "{}\n",
        "\n",
        "### Output:\n",
        "{}\n",
        "\"\"\"\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token\n",
        "\n",
        "def formatting_prompts_func(examples):\n",
        "    question = examples[\"question\"]\n",
        "    solution=examples[\"solution\"]\n",
        "    output= examples[\"is_correct\"]\n",
        "    texts = []\n",
        "    for instruction, input, output in zip(question,solution, output):\n",
        "        text = training_prompt.format(instruction, input, output) + EOS_TOKEN\n",
        "        texts.append(text)\n",
        "    return { \"text\" : texts, }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zaycfhxv3bbM"
      },
      "source": [
        "## Competition dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iIMvckLa3bbM"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"ad6398/nyu-dl-teach-maths-comp\")\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "emrZLAjk3bbM"
      },
      "outputs": [],
      "source": [
        "# Set the train dataset to the train split\n",
        "train_dataset = dataset['train']\n",
        "\n",
        "# Shuffle the training dataset\n",
        "shuffled_train = train_dataset.shuffle(seed=42)\n",
        "\n",
        "# Select the first 2,000 samples for validation\n",
        "validation_set = shuffled_train.select(range(2000))\n",
        "\n",
        "# Select the next 80,000 samples for the new training set\n",
        "# We also retrained on the next 82,000 after this to get better results\n",
        "# which lead to about 2% improvement in accuracy\n",
        "training_set = shuffled_train.select(range(2000, 82000))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qAfLnnS43bbM"
      },
      "outputs": [],
      "source": [
        "training_set = training_set.map(formatting_prompts_func, batched = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBUCC1MO3bbM"
      },
      "source": [
        "## Load model and wrap with LoRA adapters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WLshbrEW3bbM"
      },
      "outputs": [],
      "source": [
        "# We tried many different combinations of r, from 1 to 128\n",
        "# and tried keeping lora alpha 2*r, r and r/2\n",
        "# r = 32, and lora_alpha = r/2 did great!\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 32,\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0,\n",
        "    bias = \"none\",\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        "    random_state = 500,\n",
        "    use_rslora = False,\n",
        "    loftq_config = None,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ugZX_c2u3bbN"
      },
      "source": [
        "## SFT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YmOjHg3Z3bbN"
      },
      "outputs": [],
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = training_set,\n",
        "    dataset_text_field = \"text\", # The column in the dataset that contains the pre-formatted text to be trained on\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc = 2, # The number of CPU processes to use for tokenizing and preparing the data. (Speeds up preprocessing)\n",
        "    packing = False, # maximizes the number of tokens per batch by packing multiple samples together\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 5,\n",
        "        num_train_epochs = 1,\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = not is_bfloat16_supported(),\n",
        "        bf16 = is_bfloat16_supported(),\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"cosine\", # improved accuracy over linear\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "        report_to = \"none\",\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5dIyPUw3bbN"
      },
      "source": [
        "## TRAINING\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i3ar9vKI3bbN"
      },
      "outputs": [],
      "source": [
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZvZDSuz3bbN"
      },
      "source": [
        "## SAVING MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FDJf77N56fEA"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1E7h0fVa6llh"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Define the path to save the model checkpoint in Google Drive\n",
        "save_path = \"/content/drive/MyDrive/soham-model\"\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "os.makedirs(save_path, exist_ok=True)\n",
        "\n",
        "# Save the model and tokenizer\n",
        "trainer.model.save_pretrained(save_path)\n",
        "tokenizer.save_pretrained(save_path)\n",
        "\n",
        "print(f\"Model checkpoint and tokenizer saved to: {save_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_86pRXw73bbN"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "import os\n",
        "from IPython.display import FileLink\n",
        "\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "os.makedirs(save_path, exist_ok=True)\n",
        "\n",
        "# Save the model and tokenizer locally in the Kaggle environment\n",
        "model.save_pretrained(save_path)\n",
        "tokenizer.save_pretrained(save_path)\n",
        "\n",
        "# Compress the saved model directory into a ZIP file\n",
        "shutil.make_archive(\"soham-model-updated\", 'zip', save_path\n",
        "                    )\n",
        "\n",
        "# Generate a download link for the ZIP file\n",
        "# display(FileLink(r'lora_model_50.zip'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TYyNbctD3bbN"
      },
      "source": [
        "## INFERENCE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UaaiELF63bbN"
      },
      "source": [
        "## INFERENCE PROMPT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yP8WZNDH3bbN"
      },
      "outputs": [],
      "source": [
        "# improved inference prompt\n",
        "inference_prompt = \"\"\"You are a meticulous math expert. Your goal is to verify the *entire reasoning* of the solution, not just the final answer.\n",
        "Read the problem, then carefully analyze each step of the solution for logical errors, calculation mistakes, or incorrect reasoning.\n",
        "Based on your step-by-step analysis, determine if the solution is correct.\n",
        "\n",
        "Answer with ONLY True or False.\n",
        "\n",
        "### Problem:\n",
        "{}\n",
        "\n",
        "### Solution:\n",
        "{}\n",
        "\n",
        "### Output:\n",
        "\"\"\"\n",
        "\n",
        "# Function to format prompts for inference\n",
        "# just like training prompt but without the output filled in\n",
        "def formatting_prompts_func_inference(examples):\n",
        "    question = examples[\"question\"]\n",
        "    solution=examples[\"solution\"]\n",
        "    output= examples[\"is_correct\"]\n",
        "    texts = []\n",
        "    for instruction, input, output in zip(question,solution, output):\n",
        "        text = inference_prompt.format(instruction, input)\n",
        "        texts.append(text)\n",
        "    return { \"text\" : texts, }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOcAP3MA3bbO"
      },
      "source": [
        "## VALIDATION DATASET\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iM-x2Oee3bbO"
      },
      "outputs": [],
      "source": [
        "validation_dataset = validation_set.map(formatting_prompts_func_inference, batched = True,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zEB6wCdp3bbO"
      },
      "outputs": [],
      "source": [
        "# Running inference on single validation sample\n",
        "\n",
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "example_no=0\n",
        "\n",
        "input_prompt=validation_dataset['text'][example_no]\n",
        "\n",
        "print(\"Input Promt:\\n\", input_prompt)\n",
        "inputs = tokenizer(\n",
        "[\n",
        "    input_prompt\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "input_shape = inputs['input_ids'].shape\n",
        "input_token_len = input_shape[1] # 1 because of batch\n",
        "outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\n",
        "# you can get the whole generated text by uncommenting the below line\n",
        "# text_generated = tokenizer.batch_decode([outputs, skip_special_tokens=True)\n",
        "\n",
        "response = tokenizer.batch_decode([outputs[0][input_token_len:]], skip_special_tokens=True)\n",
        "response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mZUvnARg3bbO"
      },
      "outputs": [],
      "source": [
        "## Running inference in full Validation set\n",
        "\n",
        "final_response = []\n",
        "correct_predictions = 0  # Initialize correct predictions count\n",
        "for i in range(len(validation_dataset)):\n",
        "    FastLanguageModel.for_inference(model)\n",
        "    input_prompt=validation_dataset['text'][i]\n",
        "    inputs = tokenizer([\n",
        "          input_prompt\n",
        "      ], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "    input_shape = inputs['input_ids'].shape\n",
        "    input_token_len = input_shape[1] # 1 because of batch\n",
        "    outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\n",
        "    response = tokenizer.batch_decode([outputs[0][input_token_len:]], skip_special_tokens=True)\n",
        "    final_response.append(response[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MasXgSVJ3bbO"
      },
      "source": [
        "## VALIDATION ACCURACY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EglqdnXU3bbO"
      },
      "outputs": [],
      "source": [
        "def extract_last_true_false(s):\n",
        "    matches = re.findall(r'\\b(True|False)\\b', s, flags=re.IGNORECASE)\n",
        "    if matches:\n",
        "        last_match = matches[-1].lower()\n",
        "        return True if last_match == 'true' else False\n",
        "    else:\n",
        "        return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_RuQktSg3bbO"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "validation_prediction_list = [extract_last_true_false(s) for s in final_response]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i_FWQJqd3bbO"
      },
      "outputs": [],
      "source": [
        "validation_truth_list=dataset['validation']['is_correct']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4oxV2NfS3bbP"
      },
      "outputs": [],
      "source": [
        "# Ensure both lists have the same length\n",
        "assert len(validation_prediction_list) == len(validation_truth_list), \"Lists must have the same length.\"\n",
        "\n",
        "# Calculate the number of correct predictions\n",
        "correct_predictions = sum(\n",
        "    pred == truth for pred, truth in zip(validation_prediction_list, validation_truth_list)\n",
        ")\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = correct_predictions / len(validation_truth_list)\n",
        "\n",
        "print(f\"Validation Accuracy: {accuracy:.2%}\") # we got around 79% here!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Ry-Ptl1dWzh"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Load the official test set\n",
        "test_dataset = load_dataset(\"ad6398/nyu-dl-teach-maths-comp\", split=\"test\")\n",
        "predictions = []\n",
        "\n",
        "# improved inference prompt\n",
        "inference_prompt = \"\"\"You are a meticulous math expert. Your goal is to verify the *entire reasoning* of the solution, not just the final answer.\n",
        "Read the problem, then carefully analyze each step of the solution for logical errors, calculation mistakes, or incorrect reasoning.\n",
        "Based on your step-by-step analysis, determine if the solution is correct.\n",
        "\n",
        "Answer with ONLY True or False.\n",
        "\n",
        "### Problem:\n",
        "{}\n",
        "\n",
        "### Solution:\n",
        "{}\n",
        "\n",
        "### Output:\n",
        "\"\"\"\n",
        "\n",
        "# Function to parse the model's output\n",
        "# which can contain more text after True/False\n",
        "def parse_output(response_text):\n",
        "    output_part = response_text.split(\"Output:\\n\")[-1]\n",
        "    if 'true' in output_part.lower():\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "for example in tqdm(test_dataset):\n",
        "    question = example[\"question\"]\n",
        "    solution = example[\"solution\"]\n",
        "\n",
        "    prompt = inference_prompt.format(question, str(solution))\n",
        "    inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    # Generate the prediction\n",
        "    outputs = model.generate(**inputs, max_new_tokens=8, use_cache=True)\n",
        "    response_text = tokenizer.batch_decode(outputs)[0]\n",
        "\n",
        "    # Parse the prediction and add it to our list\n",
        "    prediction = parse_output(response_text)\n",
        "    predictions.append(prediction)\n",
        "\n",
        "# Create the submission DataFrame\n",
        "submission = pd.DataFrame({\n",
        "    'ID': range(len(predictions)),\n",
        "    'is_correct': predictions\n",
        "})\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "submission.to_csv('submission.csv', index=False)\n",
        "\n",
        "print(\"\\nSubmission file 'submission.csv' created successfully!\")\n",
        "print(\"You can now download this file and submit it to the Kaggle competition.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2jcw6c0wBcdT"
      },
      "outputs": [],
      "source": [
        "# We reloaded the model to train on the next 82,000 samples later\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = save_path,\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XsDTEw-bB9x9"
      },
      "outputs": [],
      "source": [
        "# Select the next 80,000 samples for the new training set\n",
        "new_training_set = shuffled_train.select(range(82000, 164000))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mqzesEV9CbIp"
      },
      "outputs": [],
      "source": [
        "# Map the formatting function to the new training set\n",
        "new_training_set = new_training_set.map(formatting_prompts_func, batched = True,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VE8tGTmBCosU"
      },
      "outputs": [],
      "source": [
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "08Oyp_9od2Kc"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Load the official test set\n",
        "test_dataset = load_dataset(\"ad6398/nyu-dl-teach-maths-comp\", split=\"test\")\n",
        "predictions = []\n",
        "\n",
        "# improved inference prompt\n",
        "inference_prompt = \"\"\"You are a meticulous math expert. Your goal is to verify the *entire reasoning* of the solution, not just the final answer.\n",
        "Read the problem, then carefully analyze each step of the solution for logical errors, calculation mistakes, or incorrect reasoning.\n",
        "Based on your step-by-step analysis, determine if the solution is correct.\n",
        "\n",
        "Answer with ONLY True or False.\n",
        "\n",
        "### Problem:\n",
        "{}\n",
        "\n",
        "### Solution:\n",
        "{}\n",
        "\n",
        "### Output:\n",
        "\"\"\"\n",
        "\n",
        "def parse_output(response_text):\n",
        "    output_part = response_text.split(\"Output:\\n\")[-1]\n",
        "    if 'true' in output_part.lower():\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "for example in tqdm(test_dataset):\n",
        "    question = example[\"question\"]\n",
        "    solution = example[\"solution\"]\n",
        "\n",
        "    prompt = inference_prompt.format(question, str(solution))\n",
        "    inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    # Generate the prediction\n",
        "    outputs = model.generate(**inputs, max_new_tokens=8, use_cache=True)\n",
        "    response_text = tokenizer.batch_decode(outputs)[0]\n",
        "\n",
        "    # Parse the prediction and add it to our list\n",
        "    prediction = parse_output(response_text)\n",
        "    predictions.append(prediction)\n",
        "\n",
        "# Create the submission DataFrame\n",
        "submission = pd.DataFrame({\n",
        "    'ID': range(len(predictions)),\n",
        "    'is_correct': predictions\n",
        "})\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "submission.to_csv('submission.csv', index=False)\n",
        "\n",
        "file_path = '/content/submission.csv'\n",
        "\n",
        "files.download(file_path)\n",
        "\n",
        "print(\"\\nSubmission file 'submission.csv' created successfully!\")\n",
        "print(\"You can now download this file and submit it to the Kaggle competition.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zvCKOPiQ29dQ"
      },
      "outputs": [],
      "source": [
        "save_path = \"/content/10hr-submission-model\"\n",
        "\n",
        "model.save_pretrained(save_path)\n",
        "tokenizer.save_pretrained(save_path)\n",
        "\n",
        "print(\"‚úÖ Model and tokenizer saved to\", save_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iHWv5RQk3CMU"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "from google.colab import files\n",
        "\n",
        "# üóúÔ∏è Zip the folder\n",
        "zip_path = f\"{save_path}.zip\"\n",
        "shutil.make_archive(save_path, 'zip', save_path)\n",
        "\n",
        "# üíæ Download the zip file\n",
        "files.download(zip_path)\n",
        "print(\"‚¨áÔ∏è Download started! Your model zip file is ready.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gIc2lbkz7PhV"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "zip_path = \"/content/10hr-submission-model.zip\"  \n",
        "extract_path = \"/content/10hr-submission-model\"   \n",
        "\n",
        "# Create folder if not exists\n",
        "os.makedirs(extract_path, exist_ok=True)\n",
        "\n",
        "# Unzip\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "print(f\"‚úÖ Unzipped to: {extract_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LAVFZhq76Zb4"
      },
      "outputs": [],
      "source": [
        "save_path = \"/content/10hr-submission-model\"\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = save_path,\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        ")\n",
        "\n",
        "# Prepare the loaded model for faster inference\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "print(f\"Model and tokenizer loaded from: {save_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "8tJ-fp5x7yS1"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Load the official test set\n",
        "test_dataset = load_dataset(\"ad6398/nyu-dl-teach-maths-comp\", split=\"test\")\n",
        "predictions = []\n",
        "\n",
        "# Create the prompt template for inference (no answer included)\n",
        "inference_prompt = \"\"\"You are a meticulous math expert. Your goal is to verify the *entire reasoning* of the solution, not just the final answer.\n",
        "Read the problem, then carefully analyze each step of the solution for logical errors, calculation mistakes, or incorrect reasoning.\n",
        "Based on your step-by-step analysis, determine if the solution is correct.\n",
        "\n",
        "Answer with ONLY True or False.\n",
        "\n",
        "### Problem:\n",
        "{}\n",
        "\n",
        "### Solution:\n",
        "{}\n",
        "\n",
        "### Output:\n",
        "\"\"\"\n",
        "\n",
        "def parse_output(response_text):\n",
        "    output_part = response_text.split(\"Output:\\n\")[-1]\n",
        "    if 'true' in output_part.lower():\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "for example in tqdm(test_dataset):\n",
        "    question = example[\"question\"]\n",
        "    solution = example[\"solution\"]\n",
        "\n",
        "    prompt = inference_prompt.format(question, str(solution))\n",
        "    inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    # Generate the prediction\n",
        "    outputs = model.generate(**inputs, max_new_tokens=8, use_cache=True)\n",
        "    response_text = tokenizer.batch_decode(outputs)[0]\n",
        "\n",
        "    # Parse the prediction and add it to our list\n",
        "    prediction = parse_output(response_text)\n",
        "    predictions.append(prediction)\n",
        "\n",
        "# Create the submission DataFrame\n",
        "submission = pd.DataFrame({\n",
        "    'ID': range(len(predictions)),\n",
        "    'is_correct': predictions\n",
        "})\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "submission.to_csv('submission.csv', index=False)\n",
        "\n",
        "file_path = '/content/submission.csv'\n",
        "\n",
        "files.download(file_path)\n",
        "\n",
        "print(\"\\nSubmission file 'submission.csv' created successfully!\")\n",
        "print(\"You can now download this file and submit it to the Kaggle competition.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 30787,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
